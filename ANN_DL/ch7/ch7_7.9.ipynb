{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.9 Hands-On Back Propagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2) (2000,)\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 2000 # number of sampling points\n",
    "TEST_SIZE = 0.3 # testing ratio\n",
    "# Use make_moons function to generate data set\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "# Split traning and testing data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot\n",
    "def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n",
    "    if (dark):\n",
    "        plt.style.use('dark_background')\n",
    "    else:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "    plt.title(plot_name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    if(XX is not None and YY is not None and preds is not None):\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n",
    "        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "    # Use color to distinguish labels\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='none')\n",
    "    plt.savefig('dataset.svg')\n",
    "    plt.close()\n",
    "# Make distribution plot\n",
    "make_plot(X, y, \"Classification Dataset Visualization \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9.2 Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # Fully connected layer\n",
    "    def __init__(self, n_input, n_neurons, activation=None, weights=None, bias=None):\n",
    "        \"\"\"\n",
    "        :param int n_input: input nodes\n",
    "        :param int n_neurons: output nodes\n",
    "        :param str activation: activation function\n",
    "        :param weights: weight vectors\n",
    "        :param bias: bias vectors\n",
    "        \"\"\"\n",
    "        # Initialize weights through Normal distribution\n",
    "        self.weights = weights if weights is not None else np.random.randn(n_input, n_neurons) * np.sqrt(1 / n_neurons)\n",
    "        self.bias = bias if bias is not None else np.random.rand(n_neurons) * 0.1\n",
    "        self.activation = activation # activation function, e.g. ’sigmoid’\n",
    "        self.last_activation = None # output of activation function o\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "    def activate(self, x):\n",
    "        # Forward propagation function\n",
    "        r = np.dot(x, self.weights) + self.bias # X@W+b\n",
    "        # Get output through activation function\n",
    "        self.last_activation = self._apply_activation(r)\n",
    "        return self.last_activation\n",
    "    def _apply_activation(self, r):\n",
    "        # Calculate output of activation function\n",
    "        if self.activation is None:\n",
    "            return r # No activation function\n",
    "        # ReLU\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(r, 0)\n",
    "        # tanh\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "        # sigmoid\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-r))\n",
    "        return r\n",
    "    def apply_activation_derivative(self, r):\n",
    "        # Calculate the derivative of activation functions\n",
    "        # If no activation function, derivative is 1\n",
    "        if self.activation is None:\n",
    "            return np.ones_like(r)\n",
    "        # ReLU\n",
    "        elif self.activation == 'relu':\n",
    "            grad = np.array(r, copy=True)\n",
    "            grad[r > 0] = 1.\n",
    "            grad[r <= 0] = 0.\n",
    "            return grad\n",
    "        # tanh\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "        # Sigmoid\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "        return r\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9.3 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # Neural Network Class\n",
    "    def __init__(self):\n",
    "        self._layers = [] # list of network class\n",
    "    def add_layer(self, layer):\n",
    "        # Add layers\n",
    "        self._layers.append(layer)\n",
    "    def feed_forward(self, X):\n",
    "        # Forward calculation\n",
    "        for layer in self._layers:\n",
    "            # Loop through every layer\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        # Back propagation\n",
    "        # Get result of forward calculation\n",
    "        output = self.feed_forward(X)\n",
    "        for i in reversed(range(len(self._layers))): \n",
    "            # reverse loop\n",
    "            layer = self._layers[i] # get current layer\n",
    "            # If it’s output layer\n",
    "            if layer == self._layers[-1]: # output layer\n",
    "                layer.error = y - output\n",
    "                # calculate delta\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(output)\n",
    "            else: # For hidden layer\n",
    "                next_layer = self._layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                # Calculate delta\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)\n",
    "                # Update weights\n",
    "                for i in range(len(self._layers)):\n",
    "                    layer = self._layers[i]\n",
    "                    # o_i is output of previous layer\n",
    "                    o_i = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)\n",
    "                    # Gradient descent\n",
    "                    layer.weights += layer.delta * o_i.T * learning_rate\n",
    "    def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):\n",
    "        y_onehot = np.zeros((y_train.shape[0], 2))\n",
    "        y_onehot[np.arange(y_train.shape[0]), y_train] = 1\n",
    "        mses = []\n",
    "        for i in range(max_epochs): # Train 1000 epoches\n",
    "            for j in range(len(X_train)): # Train one sample per time\n",
    "                self.backpropagation(X_train[j], y_onehot[j], learning_rate)\n",
    "                if i % 10 == 0:\n",
    "                    # Print MSE Loss\n",
    "                    mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))\n",
    "                    mses.append(mse)\n",
    "                    print('Epoch: #%s, MSE: %f' % (i, float(mse)))\n",
    "                    # Print accuracy\n",
    "                    print('Accuracy: %.2f%%' % (self.accuracy(self.predict(X_test), y_test.flatten()) * 100))\n",
    "        return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer(Layer(2, 25, 'sigmoid')) # Hidden layer 1, 2=>25\n",
    "nn.add_layer(Layer(25, 50, 'sigmoid')) # Hidden layer 2, 25=>50\n",
    "nn.add_layer(Layer(50, 25, 'sigmoid')) # Hidden layer 3, 50=>25\n",
    "nn.add_layer(Layer(25, 2, 'sigmoid')) # Hidden layer, 25=>2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9.4 Network Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9.5 Network Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# After training 1000 Epochs, the accuracy rate obtained on 600 samples in the test set is\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# 99.17%\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# code:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m nn\u001b[39m.\u001b[39;49mtrain(X_train, X_test, y_train, y_test, \u001b[39m0.01\u001b[39;49m, \u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[19], line 44\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs): \u001b[39m# Train 1000 epoches\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train)): \u001b[39m# Train one sample per time\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagation(X_train[j], y_onehot[j], learning_rate)\n\u001b[0;32m     45\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m             \u001b[39m# Print MSE Loss\u001b[39;00m\n\u001b[0;32m     47\u001b[0m             mse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39msquare(y_onehot \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(X_train)))\n",
      "Cell \u001b[1;32mIn[19], line 37\u001b[0m, in \u001b[0;36mNeuralNetwork.backpropagation\u001b[1;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[0;32m     35\u001b[0m o_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(X \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layers[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlast_activation)\n\u001b[0;32m     36\u001b[0m \u001b[39m# Gradient descent\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m layer\u001b[39m.\u001b[39mweights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mdelta \u001b[39m*\u001b[39;49m o_i\u001b[39m.\u001b[39;49mT \u001b[39m*\u001b[39m learning_rate\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "# After training 1000 Epochs, the accuracy rate obtained on 600 samples in the test set is\n",
    "\n",
    "# 99.17%\n",
    "# code:\n",
    "nn.train(X_train, X_test, y_train, y_test, 0.01, 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46a5e820fae5c01f8ab8f3b920045ce887673a37bd422c73202f4e116d9bb3c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
